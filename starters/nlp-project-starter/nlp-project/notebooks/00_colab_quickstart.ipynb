{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Starter — Colab/Kaggle Quickstart\n",
    "\n",
    "> Author : Badr TAJINI\n",
    "\n",
    "**Academic year:** 2025–2026  \n",
    "**School:** ECE  \n",
    "**Course:** Machine Learning & Deep Learning 2 \n",
    "\n",
    "---\n",
    "\n",
    "Welcome! This notebook walks you through the AG News text-classification starter on a free GPU runtime. Follow the steps in order to check the environment, install dependencies, and optionally train/evaluate the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Project files\n",
    "\n",
    "- Option A: `git clone <your_repo_url> nlp-project`\n",
    "- Option B: Upload the `nlp-project` folder via the sidebar (it should appear as `/content/nlp-project` in Colab).\n",
    "\n",
    "Run the next cells afterward; they will raise helpful errors if the folder is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick checklist before running code\n",
    "- Switch the runtime to **GPU (T4)** first.\n",
    "  - Colab: `Runtime → Change runtime type → GPU → Save`.\n",
    "  - Kaggle: gear icon → enable **Accelerator**, choose `T4 x1`.\n",
    "- Wait for the runtime to reconnect.\n",
    "- Run each cell from top to bottom. If a cell errors, fix the issue, then rerun that same cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run a cell\n",
    "- Click the ▶️ button on the left of a cell, or press **Shift+Enter** (Colab) / **Ctrl+Enter** (Kaggle).\n",
    "- A cell is finished when a number like `[1]` appears on the left.\n",
    "- Do not skip cells; the later ones depend on the setup above them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 — Confirm the GPU is ready\n",
    "Run the next cell. You should see a table with GPU details (name + memory). If you see `nvidia-smi unavailable`, the runtime is still on CPU—go back and switch it to GPU, then rerun this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 14 21:18:56 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 577.03                 Driver Version: 577.03         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P8              2W /   50W |    2839MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           21796      C   ...3\\envs\\pytorch_gpu\\python.exe      N/A      |\n",
      "|    0   N/A  N/A           22820    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A           27432    C+G   ...Browser\\Application\\brave.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi || echo \"nvidia-smi unavailable (CPU runtime)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — Point the notebook at the project folder\n",
    "This cell makes sure the notebook is executing inside the `nlp-project` directory.\n",
    "If it raises a `FileNotFoundError`, double-check where you uploaded/cloned the folder, adjust the path, and rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\lucas\\End-to-End-Deep-Learning-Systems\\End-to-End-Deep-Learning-Systems\\starters\\nlp-project-starter\\nlp-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent.resolve()\n",
    "elif PROJECT_ROOT.name == \"content\":\n",
    "    candidate = PROJECT_ROOT / \"nlp-project\"\n",
    "    if candidate.exists():\n",
    "        PROJECT_ROOT = candidate.resolve()\n",
    "\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate project root at {PROJECT_ROOT}. Upload or clone nlp-project before proceeding.\"\n",
    "    )\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 — Install the project requirements\n",
    "This command reads `requirements.txt` and installs the exact package versions used locally. Expect a lot of output; that's normal. If installation fails, run the cell again before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 2)) (4.4.1)\n",
      "Requirement already satisfied: torchmetrics==1.4.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 6)) (2.3.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchmetrics==1.4.0->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchmetrics==1.4.0->-r requirements.txt (line 3)) (0.15.2)\n",
      "Requirement already satisfied: pretty-errors==1.2.25 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchmetrics==1.4.0->-r requirements.txt (line 3)) (1.2.25)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pretty-errors==1.2.25->torchmetrics==1.4.0->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (0.70.18)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets->-r requirements.txt (line 2)) (1.1.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 2)) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 2)) (0.20.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1.0.0->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lucas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 2)) (8.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install project dependencies listed in requirements.txt\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 — Run the smoke test\n",
    "This quick check downloads AG News (first run only), builds the vocabulary, and runs one mini-batch through the LSTM. It saves `outputs/smoke_metrics.json` so you know the pipeline works.\n",
    "If the cell reports a network/download issue, wait a few seconds and rerun it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_loaders.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m smoke_check\n\u001b[1;32m----> 3\u001b[0m smoke_path \u001b[38;5;241m=\u001b[39m \u001b[43msmoke_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_smoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigs/nlp_agnews.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(smoke_path\u001b[38;5;241m.\u001b[39mread_text())\n",
      "File \u001b[1;32m~\\End-to-End-Deep-Learning-Systems\\End-to-End-Deep-Learning-Systems\\starters\\nlp-project-starter\\nlp-project\\src\\smoke_check.py:52\u001b[0m, in \u001b[0;36mrun_smoke\u001b[1;34m(cfg_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[0;32m     46\u001b[0m         params,\n\u001b[0;32m     47\u001b[0m         lr\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     48\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 52\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     53\u001b[0m toks, lens, y \u001b[38;5;241m=\u001b[39m (t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[0;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\pytorch_gpu\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'build_loaders.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "from src import smoke_check\n",
    "\n",
    "smoke_path = smoke_check.run_smoke(\"configs/nlp_agnews.yaml\")\n",
    "print(smoke_path.read_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review smoke-test output\n",
    "- Confirm the previous cell printed a JSON block (loss, batch size, seq_len).\n",
    "- You should now see `outputs/smoke_metrics.json` in the file browser on the left.\n",
    "- Only need a quick check? You can stop here. Ready for full training? Continue to Section 2.\n",
    "- If anything failed, read the error message, fix the issue, and rerun the smoke cell before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full training run (optional)\n",
    "Only run these cells when you want the complete experiment. On the first run, downloading the dataset and building the vocab may take a couple of minutes.\n",
    "\n",
    "**Before running:**\n",
    "1. Open `configs/nlp_agnews.yaml` (File → Open) if you want to tweak hyperparameters (epochs, learning rate, etc.).\n",
    "2. Ensure the runtime still shows a GPU connection.\n",
    "3. Close other heavy browser tabs to free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "train:   0%|          | 0/1688 [00:00<?, ?it/s]\n",
      "                                               \n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "train.py 101 <module>\n",
      "main(args.config)\n",
      "\n",
      "train.py 78 main\n",
      "tr_loss = train_one_epoch(model, train_loader, crit, opt, device)\n",
      "\n",
      "train.py 17 train_one_epoch\n",
      "for toks, lens, y in tqdm(loader, desc=\"train\", leave=False):\n",
      "\n",
      "std.py 1181 __iter__\n",
      "for obj in iterable:\n",
      "\n",
      "dataloader.py 484 __iter__\n",
      "return self._get_iterator()\n",
      "\n",
      "dataloader.py 415 _get_iterator\n",
      "return _MultiProcessingDataLoaderIter(self)\n",
      "\n",
      "dataloader.py 1138 __init__\n",
      "w.start()\n",
      "\n",
      "process.py 121 start\n",
      "self._popen = self._Popen(self)\n",
      "\n",
      "context.py 224 _Popen\n",
      "return _default_context.get_context().Process._Popen(process_obj)\n",
      "\n",
      "context.py 336 _Popen\n",
      "return Popen(process_obj)\n",
      "\n",
      "popen_spawn_win32.py 93 __init__\n",
      "reduction.dump(process_obj, to_child)\n",
      "\n",
      "reduction.py 60 dump\n",
      "ForkingPickler(file, protocol).dump(obj)\n",
      "\n",
      "AttributeError:\n",
      "Can't pickle local object 'build_loaders.<locals>.<lambda>'\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "<string> 1 <module>\n",
      "\n",
      "\n",
      "spawn.py 116 spawn_main\n",
      "exitcode = _main(fd, parent_sentinel)\n",
      "\n",
      "spawn.py 126 _main\n",
      "self = reduction.pickle.load(from_parent)\n",
      "\n",
      "EOFError:\n",
      "Ran out of input\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the main configuration file. Expect visible progress bars.\n",
    "# The first epochs may start slowly while the dataset finishes downloading.\n",
    "!python src/train.py --config configs/nlp_agnews.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\End-to-End-Deep-Learning-Systems\\End-to-End-Deep-Learning-Systems\\starters\\nlp-project-starter\\nlp-project\\src\\evaluate.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "evaluate.py 61 <module>\n",
      "main(a.config, a.ckpt)\n",
      "\n",
      "_contextlib.py 116 decorate_context\n",
      "return func(*args, **kwargs)\n",
      "\n",
      "evaluate.py 19 main\n",
      "ckpt = torch.load(ckpt_path, map_location=device)\n",
      "\n",
      "serialization.py 1319 load\n",
      "with _open_file_like(f, \"rb\") as opened_file:\n",
      "\n",
      "serialization.py 659 _open_file_like\n",
      "return _open_file(name_or_buffer, mode)\n",
      "\n",
      "serialization.py 640 __init__\n",
      "super().__init__(open(name, mode))\n",
      "\n",
      "FileNotFoundError:\n",
      "2\n",
      "No such file or directory\n",
      "outputs/best.pt\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best checkpoint produced during training.\n",
    "# This prints validation/test metrics and writes eval.json to the outputs folder.\n",
    "!python src/evaluate.py --config configs/nlp_agnews.yaml --ckpt outputs/best.pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — What should I see now?\n",
    "- `outputs/best.pt`: saved checkpoint with the trained weights and vocabulary.\n",
    "- `outputs/log.csv`: training history (loss/accuracy/F1 per epoch).\n",
    "- `outputs/metrics.json`: summary of the best validation F1.\n",
    "- `outputs/eval.json`: validation (and test, if available) accuracy/F1.\n",
    "If any of these files are missing, scroll up to check for errors in the training/evaluation cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mirror this workflow for other runs\n",
    "1. Duplicate this notebook and rename it for your custom dataset (e.g., `01_custom_dataset.ipynb`).\n",
    "2. Edit the install cell if you add new packages to `requirements.txt`.\n",
    "3. Replace the config path with your own YAML file (CSV dataset, different splits, etc.).\n",
    "4. Use `python src/predict.py --ckpt outputs/best.pt --texts texts.json` to score custom sentences once training finishes.\n",
    "\n",
    "Following the same structure—GPU check → install → smoke test → full run—keeps your experiments tidy and reproducible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
